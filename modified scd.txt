import sys
import boto3
from datetime import datetime
from awsglue.transforms import *
from delta.tables import *
from pyspark.sql.functions import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job


sc = SparkContext.getOrCreate()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)

args = getResolvedOptions(sys.argv, ['JOB_NAME', 'raw_s3_path', 'stage_s3_path',
                          'processed_s3_bucket_name', 'processed_s3_prefix'])

# Parameters
cdc_date = datetime.now().strftime("%Y/%m/%d")
raw_s3_path = args['raw_s3_path']
stage_s3_path = args['stage_s3_path']
processed_s3_bucket_name = args['processed_s3_bucket_name']
processed_s3_prefix = args['processed_s3_prefix']
processed_s3_path = f"s3://{processed_s3_bucket_name}/{processed_s3_prefix}"


def s3_object_cnt(bucket_name, prefix):
    """
    This function will return object count in specified s3 prefix.
    """
    s3_resource = boto3.resource('s3')
    bucket = s3_resource.Bucket(bucket_name)
    count = bucket.objects.filter(Prefix=prefix)
    cnt = len(list(count))
    print(f'objects count in {bucket_name} and {prefix} prefix is {cnt} ')
    return cnt


def add_emp_key(df):
    """
    This function will genrate hash key using sha2 and adds new column emp_key to dataframe.
    """
    return df.withColumn("emp_key", sha2(concat_ws("||", col("cntrct_id"), col("cntrct_reqstn_nbr"), col("cntrct_rels_nbr")), 256))


def create_deltalake_tbl(df, s3_path):
    """
    This function will write dataframe to S3 in deltalake format.
    """
    df.write.format("delta").mode("append").save(s3_path)


def create_stage_deltalake_tbl(df, s3_path):
    """
    This function will overwrites exsiting data in s3 prefix and write dataframe to S3 in deltalake format.
    """
    df.write.format("delta").mode("overwrite").option(
        "mergeSchema", "true").save(s3_path)
    return DeltaTable.forPath(spark, s3_path)


def contract_capture_changes(stage_tbl, base_tbl):
    """
    This function will identify changes between source data and target deltalake dataset.

    """
    # join base and stage tables to identify updates
    updates_df = stage_tbl.toDF().alias("stage")\
        .join(base_tbl.alias("base"), "cntrct_id", "left")\
        .filter((col('base.emp_key').isNull()) | (col('base.emp_key') != col('stage.emp_key')))

    cols = ['cntrct_id', 'cntrct_rels_nbr', 'cntrct_reqstn_nbr', 'emp_key', 'del_ind']
    stage_updates = updates_df.select(
        'stage.*').withColumn("del_ind", lit(False)).select(*cols)
    base_updates = updates_df.select('base.*').drop('isCurrent', 'eff_strt_dttm', 'eff_end_dttm').select(*cols)

    # perform union
    union_updates = base_updates.union(
        stage_updates).filter(col('emp_key').isNotNull())

    # identify deleted records
    base_tbl.alias('base').join(
        stage_tbl.toDF().alias('stage'), 'cntrct_id', 'anti')

    drop_del_cols = ['del_ind', 'eff_start_dttm', 'eff_end_dttm', 'isCurrent']
    del_df = base_tbl.alias('base')\
        .join(stage_tbl.toDF().alias('stage'), 'cntrct_id', 'anti')\
        .drop(*drop_del_cols)\
        .withColumn("merge_del_ind", lit(True))\
        .withColumnRenamed('merge_del_ind', 'del_ind')

    union_updates_dels = union_updates.union(del_df)

    return union_updates_dels


def contract_delta_records(base_tbl, union_updates_dels):
    """
    This function will identify delete records and sets delete_flag to true and updates end_date.
    """
    delete_join_cond = "contract.cntrct_id=contractUpdates.cntrct_id and contract.emp_key = contractUpdates.emp_key"
    delete_cond = "contract.emp_key == contractUpdates.emp_key and contract.isCurrent = true and contractUpdates.del_ind = true"

    base_tbl.alias("contract").merge(union_updates_dels.alias("contractUpdates"), delete_join_cond).whenMatchedUpdate(
        condition=delete_cond, set={"isCurrent": "false", "eff_end_dttm": current_date(), "del_ind": "true"}).execute()


def contract_upsert_records(base_tbl, union_updates_dels):
    """
    This function will identify insert/update records and performs merge operation on exsiting deltalake dataset.
    """
    upsert_cond = "contract.cntrct_id=contractUpdates.cntrct_id and contract.emp_key = contractUpdates.emp_key and contract.isCurrent = true"
    upsert_update_cond = "contract.isCurrent = true and contractUpdates.del_ind = false"

    base_tbl.alias("contract").merge(union_updates_dels.alias("contractUpdates"), upsert_cond).whenMatchedUpdate(condition=upsert_update_cond, set={"isCurrent": "false", "eff_end_dttm": current_date()}).whenNotMatchedInsert(
        values={"isCurrent": "true", "cntrct_id": "contractUpdates.cntrct_id", "cntrct_rels_nbr": "contractUpdates.cntrct_rels_nbr", "cntrct_reqstn_nbr": "contractUpdates.cntrct_reqstn_nbr", "emp_key": "contractUpdates.emp_key", "eff_start_dttm": current_date(), "del_ind":  "contractUpdates.del_ind", "eff_end_dttm": "9999/12/31"}).execute()


def contract_scd(raw_s3_path, stage_s3_path, processed_s3_path):
    """
    This function will perform scd type 2 on deltalake dataset on s3 for employees sample dataset. 
    """
    srcDyf = glueContext.create_dynamic_frame.from_options(format_options={"multiline": False}, connection_type="s3", format="parquet", connection_options={
                                                           "paths": [raw_s3_path], "recurse": True, }, transformation_ctx="srcDyf")

    srcDF = add_emp_key(srcDyf.toDF())

    if s3_object_cnt(processed_s3_bucket_name, processed_s3_prefix) != 0:
        print(f'Deltalake contract data already exists. started loading data ...')

        # delta lake satge table for input data source.
        stage_tbl = create_stage_deltalake_tbl(srcDF, stage_s3_path)

        # read delta lake base table for current records
        base_tbl = DeltaTable.forPath(spark, processed_s3_path)

        base_tbl_df = base_tbl.toDF().where((col('isCurrent') == "true")
                                            & (col('del_ind') == "false"))

        union_updates_dels = contract_capture_changes(stage_tbl, base_tbl_df)

        # perform soft deletes
        contract_delta_records(base_tbl, union_updates_dels)

        # refresh base table and perform merge operation
        base_tbl = DeltaTable.forPath(spark, processed_s3_path)

        contract_upsert_records(base_tbl, union_updates_dels)

    else:
        print(f'Delta lake contract data  dosent exists. Performing inital data load ...')

        srcDF = srcDF.withColumn("eff_start_dttm", current_date()).withColumn("eff_end_dttm", lit("9999/12/31").cast(
            StringType())).withColumn("isCurrent", lit(True)).withColumn("del_ind", lit(False))

        create_deltalake_tbl(srcDF, processed_s3_path)


contract_scd(raw_s3_path, stage_s3_path, processed_s3_path)


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Data lake resources – The S3 buckets scd-blog-landing-xxxx and scd-blog-processed-xxxx (referred to as scd-blog-landing and scd-blog-processed in the subsequent sections in this post)
Sample records generator Lambda function – SampleDataGenaratorLambda-<CloudFormation Stack Name> (referred to as SampleDataGeneratorLambda)
AWS Glue Data Catalog database – deltalake_xxxxxx (referred to as deltalake)
AWS Glue Delta job – <CloudFormation-Stack-Name>-src-to-processed (referred to as src-to-processed)


s3://scd-blog-landing-7ab5c800/dataset/employee/fake_emp_data.json
