import sys
import boto3
from datetime import datetime
from awsglue.transforms import *
from delta.tables import *
from pyspark.sql.functions import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

sc = SparkContext.getOrCreate()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)

args = getResolvedOptions(sys.argv, ['JOB_NAME', 'raw_s3_path', 'stage_s3_path',
                          'processed_s3_bucket_name', 'processed_s3_prefix'])

# Parameters
cdc_date = datetime.now().strftime("%Y/%m/%d")
raw_s3_path = args['raw_s3_path']
stage_s3_path = args['stage_s3_path']
processed_s3_bucket_name = args['processed_s3_bucket_name']
processed_s3_prefix = args['processed_s3_prefix']
processed_s3_path = f"s3://{processed_s3_bucket_name}/{processed_s3_prefix}"

def s3_object_cnt(bucket_name, prefix):
    """
    This function will return object count in specified s3 prefix.
    """
    s3_resource = boto3.resource('s3')
    bucket = s3_resource.Bucket(bucket_name)
    count = bucket.objects.filter(Prefix=prefix)
    cnt = len(list(count))
    print(f'objects count in {bucket_name} and {prefix} prefix is {cnt} ')
    return cnt

def add_hash_key(df):
    """
    This function will genrate hash key using sha2 and adds new column emp_key to dataframe.
    """
    return df.withColumn("hash_key", sha2(concat_ws("||", col("cntrct_id"), col("cntrct_reles_nbr"), col("cntrct_reqstn_nbr"), col("addr_cd"), col("addr_bill_cd")), 256))

def create_deltalake_tbl(df, s3_path):
    """
    This function will write dataframe to S3 in deltalake format.
    """
    df.write.format("delta").mode("append").save(s3_path)

   
def create_stage_deltalake_tbl(df, s3_path):
    """
    This function will overwrites exsiting data in s3 prefix and write dataframe to S3 in deltalake format.
    """
    df.write.format("delta").mode("overwrite").option(
        "mergeSchema", "true").save(s3_path)
    return DeltaTable.forPath(spark, s3_path)   


def contract_capture_changes(stage_tbl, base_tbl):
    """
    This function will identify changes between source data and target deltalake dataset.

    """
    # join base and stage tables to identify updates
    updates_df = stage_tbl.toDF().alias("stage")\
        .join(base_tbl.alias("base"), "cntrct_id", "left")\
        .filter((col('base.hash_key').isNull()) | (col('base.hash_key') != col('stage.hash_key')))

    cols = ['cntrct_id', 'cntrct_reles_nbr', 'cntrct_reqstn', 'addr_cd',
            'addr_bill_cd', 'hash_key', 'del_ind']
    stage_updates = updates_df.select(
        'stage.*').withColumn("del_ind", lit(False)).select(*cols)
    base_updates = updates_df.select('base.*').drop('isCurrent', 'eff_strt_dttm', 'eff_end_dttm').select(*cols)

    # perform union
    union_updates = base_updates.union(
        stage_updates).filter(col('hash_key').isNotNull())

    # identify deleted records
    base_tbl.alias('base').join(
        stage_tbl.toDF().alias('stage'), 'cntrct_id', 'anti')

    drop_del_cols = ['del_ind', 'eff_strt_dttm', 'eff_end_dttm', 'isCurrent']
    del_df = base_tbl.alias('base')\
        .join(stage_tbl.toDF().alias('stage'), 'cntrct_id', 'anti')\
        .drop(*drop_del_cols)\
        .withColumn("merge_del_ind", lit(True))\
        .withColumnRenamed('merge_del_ind', 'del_ind')

    union_updates_dels = union_updates.union(del_df)

    return union_updates_dels


def contract_delta_records(base_tbl, union_updates_dels):
    """
    This function will identify delete records and sets del_ind to true and updates end_date.
    """
    delete_join_cond = "contract.cntrct_id=contractUpdates.cntrct_id and contract.hash_key = contractUpdates.hash_key"
    delete_cond = "contract.hash_key == contractUpdates.hash_key and contract.isCurrent = true and contractUpdates.del_ind = true"

    base_tbl.alias("contract").merge(union_updates_dels.alias("contractUpdates"), delete_join_cond).whenMatchedUpdate(
        condition=delete_cond, set={"isCurrent": "false", "eff_end_dttm": current_date(), "del_ind": "true"}).execute()
    

def contract_upsert_records(base_tbl, union_updates_dels):
    """
    This function will identify insert/update records and performs merge operation on exsiting deltalake dataset.
    """
    upsert_cond = "contract.cntrct_id=contractUpdates.cntrct_id and contract.hash_key = contractUpdates.hash_key and contract.isCurrent = true"
    upsert_update_cond = "contract.isCurrent = true and contractUpdates.del_ind = false"

    base_tbl.alias("contract").merge(union_updates_dels.alias("contractUpdates"), upsert_cond).whenMatchedUpdate(condition=upsert_update_cond, set={"isCurrent": "false", "end_date": current_date()}).whenNotMatchedInsert(
        values={"isCurrent": "true", "cntrct_id": "contractUpdates.cntrct_id", "cntrct_reles_nbr": "contractUpdates.cntrct_reles_nbr", "cntrct_reqstn_nbr": "contractUpdates.cntrct_reqstn_nbr", "addr_cd": "employeeUpdates.addr_cd", "addr_bill_cd": "contractUpdates.addr_bill_cd",
             "hash_key": "contractUpdates.hash_key", "eff_strt_dttm": current_date(), "del_ind":  "contractUpdates.del_ind", "eff_end_dttm": "9999/12/31"}).execute()    
    

 def contract_scd(raw_s3_path, stage_s3_path, processed_s3_path):
    """
    This function will perform scd type 2 on deltalake dataset on s3 for contract sample dataset. 
    """
    srcDyf = glueContext.create_dynamic_frame.from_options(format_options={"multiline": False}, connection_type="s3", format="parquet", connection_options={
                                                           "paths": [raw_s3_path], "recurse": True, }, transformation_ctx="srcDyf")

    srcDF = add_hash_key(srcDyf.toDF())

    if s3_object_cnt(processed_s3_bucket_name, processed_s3_prefix) != 0:
        print(f'Deltalake contract data already exists. started loading data ...')

        # delta lake satge table for input data source.
        stage_tbl = create_stage_deltalake_tbl(srcDF, stage_s3_path)

        # read delta lake base table for current records
        base_tbl = DeltaTable.forPath(spark, processed_s3_path)

        base_tbl_df = base_tbl.toDF().where((col('isCurrent') == "true")
                                            & (col('del_ind') == "false"))

        union_updates_dels = contract_capture_changes(stage_tbl, base_tbl_df)

        # perform soft deletes
        contract_delta_records(base_tbl, union_updates_dels)

        # refresh base table and perform merge operation
        base_tbl = DeltaTable.forPath(spark, processed_s3_path)

        contract_upsert_records(base_tbl, union_updates_dels)

    else:
        print(f'Delta lake contract data  doesnt exists. Performing inital data load ...')

        srcDF = srcDF.withColumn("eff_strt_dttm", current_date()).withColumn("eff_end_dttm", lit("9999/12/31").cast(
            StringType())).withColumn("isCurrent", lit(True)).withColumn("del_ind", lit(False))

        create_deltalake_tbl(srcDF, processed_s3_path)


contract_scd(raw_s3_path, stage_s3_path, processed_s3_path)   
